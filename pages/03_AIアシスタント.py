import streamlit as st
import pandas as pd
import requests
from datetime import datetime
import os
import openai

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•°
@st.cache_data
def load_data():
    df = pd.read_csv('sales_test_data_utf8.csv')
    df['å£²ä¸Šå¹´æœˆ'] = pd.to_datetime(df['å£²ä¸Šå¹´æœˆ'], format='%Y-%m')
    return df

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = load_data()

# LLMæ¥ç¶šè¨­å®š
LLM_URL = "https://api.openai.com"

def call_llm_api(prompt, context=""):
    """LLM APIã‚’å‘¼ã³å‡ºã™é–¢æ•°"""
    try:
        # å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
        data_summary = f"""
        å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦:
        - ç·ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df):,}ä»¶
        - æœŸé–“: {df['å£²ä¸Šå¹´æœˆ'].min().strftime('%Y-%m')} ã€œ {df['å£²ä¸Šå¹´æœˆ'].max().strftime('%Y-%m')}
        - æ‹…å½“è€…æ•°: {df['æ‹…å½“è€…'].nunique()}å
        - å•†å“æ•°: {df['å•†å“å'].nunique()}ç¨®é¡
        - é¡§å®¢æ•°: {df['é¡§å®¢å'].nunique()}ç¤¾
        - ç·å£²ä¸Š: Â¥{df['å£²ä¸Šé‡‘é¡'].sum():,}
        - ç·ç²—åˆ©: Â¥{df['ç²—åˆ©é‡‘é¡'].sum():,}
        - å¹³å‡ç²—åˆ©ç‡: {(df['ç²—åˆ©é‡‘é¡'].sum() / df['å£²ä¸Šé‡‘é¡'].sum() * 100):.1f}%
        """
        
        # æœ€æ–°ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ï¼ˆä¸Šä½10ä»¶ï¼‰
        recent_sales = df.sort_values('å£²ä¸Šå¹´æœˆ', ascending=False).head(10)[
            ['å£²ä¸Šå¹´æœˆ', 'å•†å“å', 'æ‹…å½“è€…', 'é¡§å®¢å', 'å£²ä¸Šé‡‘é¡', 'ç²—åˆ©é‡‘é¡']
        ].to_string(index=False)
        
        system_prompt = f"""
        ã‚ãªãŸã¯å„ªç§€ãªå£²ä¸Šãƒ‡ãƒ¼ã‚¿åˆ†æã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
        ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ãŸå…·ä½“çš„ãªæ•°å€¤ã‚„åˆ†æã‚’å«ã‚ã¦ã€æ´å¯Ÿã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

        {context}
        
        {data_summary}
        
        æœ€æ–°ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ï¼ˆä¸Šä½10ä»¶ï¼‰:
        {recent_sales}
        """
        
        api_key = os.environ.get("API_KEY")
        # LLM_URLãŒOpenAIã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®å ´åˆã®ã¿openaiãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ä½¿ã†ä¾‹
        if LLM_URL.startswith("https://api.openai.com"):  # OpenAIå…¬å¼APIã®å ´åˆ
            client = openai.OpenAI(api_key=api_key)
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            return response.choices[0].message.content
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚µãƒ¼ãƒãƒ¼ï¼ˆOpenAIäº’æ›APIï¼‰ã«ã¯requestsã§POST
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}"
            }
            payload = {
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 1000,
                "temperature": 0.7
            }
            response = requests.post(
                f"{LLM_URL}/v1/chat/completions",
                json=payload,
                headers=headers,
                timeout=120
            )
            if response.status_code == 200:
                result = response.json()
                return result.get('choices', [{}])[0].get('message', {}).get('content', 'å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚')
            else:
                return f"APIæ¥ç¶šã‚¨ãƒ©ãƒ¼: {response.status_code} - {response.text}"
            
    except requests.exceptions.Timeout:
        return "LLMã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã®å¿œç­”ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ120ç§’ï¼‰ã€‚ã‚µãƒ¼ãƒãƒ¼ã®å‡¦ç†ãŒé‡ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
    except requests.exceptions.ConnectionError:
        return "LLMã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ã€URLãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    except Exception as e:
        return f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

# ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸
st.title("ğŸ¤– AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")
st.markdown("---")

# APIã‚­ãƒ¼ã®ç¢ºèª
api_key = os.environ.get("API_KEY")
if not api_key or api_key == "Your_LLM_API_Key_Here":
    st.error("ğŸ”‘ APIã‚­ãƒ¼ãŒç’°å¢ƒå¤‰æ•°ã¨ã—ã¦è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ‡ãƒ—ãƒ­ã‚¤å…ˆã®ç’°å¢ƒå¤‰æ•°ã«API_KEYã‚’ç™»éŒ²ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ - ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
st.sidebar.header("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")

# æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
min_date = df['å£²ä¸Šå¹´æœˆ'].min()
max_date = df['å£²ä¸Šå¹´æœˆ'].max()
date_range = st.sidebar.date_input(
    "æœŸé–“é¸æŠ",
    value=(min_date, max_date),
    min_value=min_date,
    max_value=max_date
)

# æ‹…å½“è€…ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
all_staff = ['å…¨ã¦'] + sorted(df['æ‹…å½“è€…'].unique().tolist())
selected_staff = st.sidebar.selectbox("æ‹…å½“è€…", all_staff)

# å•†å“ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
all_products = ['å…¨ã¦'] + sorted(df['å•†å“å'].unique().tolist())
selected_product = st.sidebar.selectbox("å•†å“", all_products)

# ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨
filtered_df = df.copy()

if len(date_range) == 2:
    start_date, end_date = date_range
    filtered_df = filtered_df[
        (filtered_df['å£²ä¸Šå¹´æœˆ'].dt.date >= start_date) &
        (filtered_df['å£²ä¸Šå¹´æœˆ'].dt.date <= end_date)
    ]

if selected_staff != 'å…¨ã¦':
    filtered_df = filtered_df[filtered_df['æ‹…å½“è€…'] == selected_staff]

if selected_product != 'å…¨ã¦':
    filtered_df = filtered_df[filtered_df['å•†å“å'] == selected_product]

# ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æƒ…å ±ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¿½åŠ 
filter_context = f"""
ç¾åœ¨ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®š:
- æœŸé–“: {date_range[0]} ã€œ {date_range[1]}
- æ‹…å½“è€…: {selected_staff}
- å•†å“: {selected_product}
- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨å¾Œã®ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(filtered_df):,}ä»¶
"""

# ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
st.subheader("ğŸ’¬ AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«è³ªå•")

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
if prompt := st.chat_input("å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„..."):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # AIå¿œç­”ã‚’ç”Ÿæˆ
    with st.chat_message("assistant"):
        with st.spinner("AIãŒå›ç­”ã‚’ç”Ÿæˆä¸­..."):
            response = call_llm_api(prompt, filter_context)
            st.markdown(response)
    
    # AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
    st.session_state.messages.append({"role": "assistant", "content": response})

# ã‚¯ã‚¤ãƒƒã‚¯è³ªå•ãƒœã‚¿ãƒ³
st.subheader("ğŸš€ ã‚ˆãã‚ã‚‹è³ªå•")

col1, col2 = st.columns(2)

with col1:
    if st.button("ğŸ“ˆ å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æã—ã¦"):
        with st.spinner("åˆ†æä¸­..."):
            question = "ç¾åœ¨ã®æœŸé–“ã§ã®å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æã—ã€æˆé•·ã—ã¦ã„ã‚‹å•†å“ã‚„æ‹…å½“è€…ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
            response = call_llm_api(question, filter_context)
            st.info(response)
    
    if st.button("ğŸ‘¥ æ‹…å½“è€…åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹"):
        with st.spinner("åˆ†æä¸­..."):
            question = "æ‹…å½“è€…åˆ¥ã®å£²ä¸Šãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’åˆ†æã—ã€æœ€ã‚‚å„ªç§€ãªæ‹…å½“è€…ã¨ãã®ç‰¹å¾´ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
            response = call_llm_api(question, filter_context)
            st.info(response)

with col2:
    if st.button("ğŸ“¦ å•†å“åˆ¥åˆ†æ"):
        with st.spinner("åˆ†æä¸­..."):
            question = "å•†å“åˆ¥ã®å£²ä¸Šåˆ†æã‚’è¡Œã„ã€æœ€ã‚‚å£²ä¸ŠãŒè‰¯ã„å•†å“ã¨ç²—åˆ©ç‡ã®é«˜ã„å•†å“ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
            response = call_llm_api(question, filter_context)
            st.info(response)
    
    if st.button("ğŸ’° åç›Šæ€§åˆ†æ"):
        with st.spinner("åˆ†æä¸­..."):
            question = "å…¨ä½“çš„ãªåç›Šæ€§ã‚’åˆ†æã—ã€æ”¹å–„ç‚¹ã‚„æ¨å¥¨äº‹é …ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
            response = call_llm_api(question, filter_context)
            st.info(response)

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³
if st.button("ğŸ—‘ï¸ ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ã‚¯ãƒªã‚¢"):
    st.session_state.messages = []
    st.rerun()

# ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼è¡¨ç¤º
st.subheader("ğŸ“Š ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼")
col1, col2, col3, col4 = st.columns(4)

with col1:
    total_sales = filtered_df['å£²ä¸Šé‡‘é¡'].sum()
    st.metric("ç·å£²ä¸Šé‡‘é¡", f"Â¥{total_sales:,}")

with col2:
    total_profit = filtered_df['ç²—åˆ©é‡‘é¡'].sum()
    st.metric("ç·ç²—åˆ©é‡‘é¡", f"Â¥{total_profit:,}")

with col3:
    profit_rate = (total_profit / total_sales * 100) if total_sales > 0 else 0
    st.metric("ç²—åˆ©ç‡", f"{profit_rate:.1f}%")

with col4:
    avg_sales = filtered_df['å£²ä¸Šé‡‘é¡'].mean()
    st.metric("å¹³å‡å£²ä¸Š", f"Â¥{avg_sales:,.0f}")

# æ¥ç¶šçŠ¶æ³è¡¨ç¤º
st.sidebar.markdown("---")
st.sidebar.markdown("### ğŸ”— LLMæ¥ç¶šçŠ¶æ³")
try:
    response = requests.get(f"{LLM_URL}/health", timeout=5)
    if response.status_code == 200:
        st.sidebar.success("âœ… LLMã‚µãƒ¼ãƒãƒ¼æ¥ç¶šä¸­")
    else:
        st.sidebar.error("âŒ LLMã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼")
except:
    st.sidebar.error("âŒ LLMã‚µãƒ¼ãƒãƒ¼æœªæ¥ç¶š")
    st.sidebar.info("LLMã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã¦ãã ã•ã„") 